---
title: "Exp_008 Visualizing Crimes, rates, and spatial and temporal patterns"
date:  "2023-12-04"
author: "Coach Skufca"
output: html_notebook
---

This experiment will try to explore crime data for both spatial and temporal patterns.

I particular, I am interested in the following logic train:

* Is their a daily temporal pattern for crime?
* If so, is that pattern consistent throughout the city?
* Is there a temporal pattern for bike ridership?
* Is the temporal pattern of bike ridership different in high crime neighborhoods?
    * Can we "see" an effect
    
The anticipated causal chain we are exploring is --- high risk crime areas would suppress bike riding,
especially during times of increase street crime.



## Packages

Standards:

```{r}
library(knitr)
library(tidyverse)
library(janitor)
library(lubridate) # because we will probably see some dates
library(here) # more easily access files in your project
```

Some additional packages focuses on today's work:

```{r}
library(sf) # working with simple features - geospatial
library(tmap)
library(tidycensus)
library(gbfs)
theme_set(theme_minimal())

```

A link to a book on `tmap`: https://r-tmap.github.io/



## Using the Neighborhood Geospatial Data (using /data)

One way the DC is subdivided is by "neighborhood".

https://opendata.dc.gov/datasets/DCGIS::dc-health-planning-neighborhoods/about


I will use the GeoJSON file.  (Newer, not necessarily better, but ... a single file.  Not smaller, but ... this one is not big.)  



Data is easily readable 
```{r}
neigh=st_read(here("data_raw",
                   "DC_Health_Planning_Neighborhoods.geojson")) %>% clean_names()

class(neigh)
```
# Goal Visualization

Lets build chloropleth maps representing density of crime across the 51 neighborhoods.

* Experiment with small data to get the code running, then creep up to full size.


## Get crime data

Read ride information:

```{r message=FALSE, warning=FALSE}
crime=st_read(here("data_raw",
                   "Crime_Incidents_in_2022.geojson")) %>% clean_names()

class(crime)
```

Use a subsample to keep problem small

```{r}
crime_s=crime %>% slice_sample(n=1000)

```

```{r}
tmap_mode("view")

tm_shape(crime_s)+
  tm_dots("offense")
```

```{r}
tmap_mode("view")

tm_shape(crime_s)+
  tm_facets("shift",free.coords = FALSE)+
  tm_dots("method")
  #tm_dots("offense", legend.show = FALSE)
```



### Focus on violent crimes

I think we should pay special attention to violent crimes.

I will filter specifically for ROBBERY, SEX ABUSE, HOMICIDE, ASSAULT.  I 
realize that these are still spread throughout the city.  

```{r}
crime_v=crime |>
  filter(offense %in% c("ROBBERY", "SEX ABUSE", "HOMICIDE", "ASSAULT W/DANGEROUS WEAPON")) |>
  mutate(offense=as_factor(offense))
```

```{r}
tm_shape(crime_v)+
  tm_facets("offense")+
  tm_dots("offense", legend.show = FALSE)
```

### Varying with night?

I am curious as to whether the density pattern seems to vary between day and night.

As a "rough" visualization, let's simply separate out the time period 9PM-6AM based on 
start_date.

```{r}
crime_v2=crime_v %>% 
  drop_na(offense) |>
  mutate(tod=hour(start_date), night=(tod<6 | tod>20)) |>
  relocate(night,.before=start_date)
```

```{r}

tm_shape(crime_v2)+ 
  tm_facets("night")+
  tm_dots("offense", legend.show = FALSE)
```

Point clouds do not seem too revealing.


# to fix


```{r}
neigh2=neigh %>%
  mutate(crimes=lengths(st_intersects(.,crime_v)))


```






```{r}
neigh2 %>% 
  tm_shape()+tm_polygons(c("crimes"),alpha=.4)
```



### Normalize by population?

Let's grag census data as we did before, and join with neighborhood data,
as before, but also creating a "crime_rate" variable.



```{r}
df_cens=get_acs(geography = "tract",
                  variables=c("median_inc"="B06011_001",
                              "pop"="B01001_001",
                              "pop_black"="B02009_001"),
                  state="DC",geometry=TRUE,year=2021)  %>% 
  select(-moe) %>% 
  pivot_wider(names_from = "variable", 
              values_from = "estimate") %>%
  st_transform(4326)

df_j=st_join(df_cens,neigh2,largest=TRUE)

df1=df_j %>% select(median_inc,pop,pop_black,code) %>%
  group_by(code) %>%
  summarise(pop_n=sum(pop),
            pop_black_n=sum(pop_black), 
            adj_median_income=sum(pop*median_inc)/pop_n)

df2=left_join(neigh2,df1 %>% st_set_geometry(NULL)) |>
  mutate(crime_rate=crimes/pop_n,
         black_perc=pop_black_n/pop_n)
```


```{r}
tm_shape(df2%>% filter(code!="N0"))+tm_polygons(c("adj_median_income","crime_rate","black_perc"),alpha=.3)
```

### Normalizing by area

I hypothesized that what I really am interested in is whether crime affects behavior, 
in which case, we really need to understand "perceived crime risk".   I did a token
bit of research on this idea, but my sense is that ... a major effect would be the
"density" of crimes (how many crimes per unit area) when determining whether an area was safe.

So --- I want to recompute "crime rate" as two different variables, one normalizing by population, and the other by area.

Of course, we will need a tool to compute areas of each of our neighborhoods.


```{r}
df2=left_join(neigh2,df1 %>% st_set_geometry(NULL)) |>
  mutate(crime_rate_pop=crimes/pop_n,
         black_perc=pop_black_n/pop_n,
         shapearea=st_area(neigh2) %>% units::set_units(km^2),
         crime_rate_area=crimes/shapearea) 
  
```
```{r}
tm_shape(df2%>% filter(code!="N0"))+
  tm_polygons(c("crime_rate_pop","crime_rate_area","black_perc"),
                  alpha=.3,
          id="name")
```
### A temporal look at the crime data

Let's plot timeline of violent crimes:
```{r}
crime_v2 |>
  ggplot(aes(start_date,offense)) + geom_point() 
```
It's interesting that some crimes are old.   Let's re-visualize using the time that the crime was reported.

```{r}
crime_v2 |>
  ggplot(aes(report_dat,offense)) + geom_point()
```
Not very informative.   We are interested in the _temporal density_ of crimes, so let's use a different geometry.

```{r}

crime_v2 |>
  ggplot(aes(report_dat,color=offense)) + geom_density()
```
```{r}

crime_v2 |>
  ggplot(aes(report_dat,y=offense,fill=offense)) + 
  ggridges::geom_density_ridges(bandwidth = 3600*24*7)  + 
  theme(legend.position = "none")
```
Seems like there is time-of-year variability.

Maybe a different plot would be useful:

```{r}
library(ggbeeswarm)

crime_v2 |>
  mutate(offense=fct_recode(offense,assault.weap="ASSAULT W/DANGEROUS WEAPON")) %>%
  ggplot(aes(report_dat,y=offense,fill=offense)) + 
  geom_quasirandom(bandwidth = .05,alpha=.2)  + 
  theme(legend.position = "none") +
  scale_x_datetime(date_breaks="1 month", date_labels="%b")



```

If I zoom to a shorter time period, can I see a finer temporal pattern?

```{r}
crime_v2 |> 
  filter(report_dat >="2022-02-01" & report_dat <"2022-02-07" ) |>
  mutate(offense=fct_recode(offense,assault.weap="ASSAULT W/DANGEROUS WEAPON")) %>%
  ggplot(aes(report_dat,y=offense,fill=offense)) + 
  geom_quasirandom(bandwidth = .1,alpha=.2)  + 
  theme(legend.position = "none") +
  scale_x_datetime(date_breaks="1 day", date_labels="%d")
```

The data is pretty sparse --- like only about 10 violent crimes per day (on average).

So --- lets aggregate over many days and look at finder resolution:

```{r}
p5=crime_v2 |> 
  #filter(report_dat >="2022-02-01" & report_dat <"2022-02-07" ) |>
  mutate(offense=fct_recode(offense,assault.weap="ASSAULT W/DANGEROUS WEAPON")) %>%
  ggplot(aes(hms::as_hms(start_date)  ,y=offense,fill=offense)) + 
  geom_quasirandom(bandwidth = .00001,alpha=.2)  + 
  #geom_point(bandwidth = .05,alpha=.2)+
  #geom_jitter(width=0,alpha=.2)+
  theme(legend.position = "none") 
#p5 = plotly::ggplotly(p5)
p5
#+
#  scale_x_datetime(date_breaks="1 day", date_labels="%d")
```

Curious question - is the seasonal variation related to ACTUAL temperatures?


```{r}
library(openmeteo)
df_w=weather_history("Washington",
                    start = "2022-01-01",
                    end = "2022-12-31",
                    hourly = c("apparent_temperature",
                               "wind_speed_10m",
                               "precipitation")
)
```

```{r}

crime_v2 |>
  mutate(offense=fct_recode(offense,assault.weap="ASSAULT W/DANGEROUS WEAPON")) %>%
  ggplot(aes(report_dat,y=offense,fill=offense)) + 
  geom_quasirandom(bandwidth = .05,alpha=.2)  + 
  theme(legend.position = "none") +
  scale_x_datetime(date_breaks="1 month", date_labels="%b")

```

```{r}
df_w %>% 
  #filter(day(datetime)==18) %>%
  ggplot(aes(datetime,hourly_apparent_temperature)) +
  geom_line()


```

# Bike feed data

Let's load some data showing station locations:


```{r}
dfbc=get_gbfs_cities()
```

We are interested in `cabi` for system id.

```{r}
dfbs=get_station_information("cabi")
```

```{r}
dfbs_sf=dfbs %>% 
  st_as_sf(coords = c("lon", "lat"), crs = 4326)
```

```{r}
tm_shape(dfbs_sf)+
    tm_dots("capacity")
```

#### Realtime data

Let's pull station data:

```{r}
station_status=get_station_status("https://gbfs.capitalbikeshare.com/gbfs/2.3/gbfs.json")
```


```{r}
bike_status=get_free_bike_status("https://gbfs.capitalbikeshare.com/gbfs/2.3/gbfs.json") |>
  st_as_sf(coords = c("lon", "lat"), crs = 4326)
```

```{r}
tm_shape(bike_status)+
    tm_dots("vehicle_type_id")
```

#### Let's look at station real-time info

```{r}
sf_stations=left_join(dfbs_sf,station_status,by="station_id")
```


```{r}
tm_shape(sf_stations)+
    tm_dots("num_bikes_available",popup.vars=c("name"))
```
